\documentclass[12pt, a4paper]{article}

% --- PACOTES BÁSICOS ---
\usepackage[utf8]{inputenc} % Permite o uso de acentos e caracteres especiais
\usepackage[T1]{fontenc}    % Define a codificação da fonte
\usepackage[brazil]{babel}  % Traduz os títulos automáticos 
\usepackage{amsmath}        % Para equações matemáticas avançadas
\usepackage{graphicx}       % Para incluir imagens 
\usepackage{geometry}       % Para configurar as margens
\usepackage{hyperref}       % Para links clicáveis 

% --- CONFIGURAÇÃO DAS MARGENS ---
\geometry{
    a4paper,
    left=2.5cm,
    right=2.5cm,
    top=2.5cm,
    bottom=2.5cm
}

% --- TÍTULO DO DOCUMENTO ---
\title{Relatório do Projeto de Compressor Huffman}
\author{Kauã do Vale Ferreira \and Caio de Medeiros Trindade}
\date{\today} % Usa a data atual

\begin{document}

\maketitle % Gera o título

\section{Introdução}

Este documento detalha a implementação de um compressor e descompressor de arquivos baseado no algoritmo de Huffman, desenvolvido como parte do projeto da disciplina de Estrutura de Dados 2. O objetivo principal foi criar uma ferramenta de linha de comando em C++ capaz de comprimir arquivos de texto, com uma otimização especial para códigos-fonte de linguagens como C++, PY, Java, JS, TS.

O projeto foi dividido em dois executáveis principais:
\begin{enumerate}
    \item \textbf{Contador de Frequência}: Uma ferramenta para analisar um conjunto de arquivos e gerar uma tabela de frequência de "símbolos". Diferente de uma abordagem tradicional, os símbolos não são apenas caracteres individuais, mas também palavras-chave da linguagem (ex: \texttt{int}, \texttt{while}, \texttt{std::vector}), permitindo uma compressão mais semântica e eficiente para código.
    \item \textbf{Compressor/Descompressor}: O programa principal que utiliza uma tabela de frequência (seja uma pré-calculada ou gerada em tempo real) para construir uma Árvore de Huffman, comprimir um arquivo de entrada e, inversamente, descomprimir um arquivo para restaurar seu conteúdo original.
\end{enumerate}

\section{Análise de Complexidade}

A eficiência do compressor pode ser analisada em três etapas principais: a contagem de frequência, o processo de compressão e o de descompressão.

\subsection{Contagem de Frequência}

O processo de contagem de frequência, realizado pelo programa \texttt{contador\_frequencia}, envolve a leitura de um ou mais arquivos de entrada e a contabilização dos símbolos.
\begin{itemize}
    \item \textbf{Leitura e Tokenização}: O programa lê cada arquivo de entrada, de tamanho $N$, e o divide em tokens. A identificação de palavras-chave é feita com base em um conjunto pré-definido. Esta etapa percorre todo o conteúdo do arquivo uma vez.
    \item \textbf{Armazenamento}: As frequências são armazenadas em um \texttt{std::map}. A inserção ou atualização de um símbolo no mapa tem um custo médio de $O(\log K)$, onde $K$ é o número de símbolos únicos.
\end{itemize}
A complexidade total desta etapa é dominada pela leitura e processamento do arquivo, resultando em \textbf{$O(N \log K)$}. Como o número de símbolos únicos ($K$) é geralmente muito menor que o tamanho do arquivo ($N$), a complexidade na prática se aproxima de \textbf{$O(N)$}.

\subsection{Processo de Compressão}

A compressão, executada pelo programa \texttt{compressor}, envolve os seguintes passos:
\begin{enumerate}
    \item \textbf{Leitura da Tabela de Frequência}: Se um arquivo de tabela é fornecido, a leitura tem custo proporcional ao número de símbolos únicos, $O(K)$.
    \item \textbf{Construção da Árvore de Huffman}: A árvore é construída a partir das frequências usando uma \texttt{std::priority\_queue}. Inserir os $K$ símbolos na fila de prioridade custa $O(K \log K)$. Em seguida, o algoritmo remove os dois nós de menor frequência e insere um novo nó pai, repetindo o processo $K-1$ vezes. Cada operação (remoção/inserção) custa $O(\log K)$. Portanto, a complexidade total para construir a árvore é \textbf{$O(K \log K)$}.
    \item \textbf{Geração dos Códigos}: O programa percorre a árvore para gerar o mapa de códigos binários para cada símbolo. A complexidade desta etapa é proporcional ao tamanho da árvore, $O(K)$.
    \item \textbf{Codificação do Arquivo}: O arquivo de entrada, de tamanho $N$, é lido e tokenizado novamente (se necessário). Em seguida, cada token é substituído pelo seu código de Huffman correspondente. O custo é proporcional ao número de tokens ($T$) multiplicado pelo comprimento médio dos códigos ($L_{avg}$), resultando em $O(T \times L_{avg})$. No total, isso é proporcional ao tamanho do arquivo de saída.
\end{enumerate}
A etapa dominante é a construção da árvore, tornando a complexidade geral da compressão \textbf{$O(N + K \log K)$}.

\subsection{Processo de Descompressão}

A descompressão é um processo mais direto:
\begin{enumerate}
    \item \textbf{Desserialização da Árvore}: O programa lê o cabeçalho do arquivo comprimido para reconstruir a Árvore de Huffman. A complexidade é proporcional ao número de nós da árvore, \textbf{$O(K)$}.
    \item \textbf{Decodificação}: O restante do arquivo é lido bit a bit. Para cada bit, o programa navega na árvore (da raiz para a esquerda ou direita) até encontrar um nó-folha, que corresponde a um símbolo. Este processo é repetido até o fim do arquivo. Se o arquivo comprimido tem $M$ bits, a complexidade é \textbf{$O(M)$}.
\end{enumerate}
A complexidade da descompressão é linear em relação ao tamanho do arquivo comprimido.

\section{Análise da Taxa de Compressão}

Para avaliar a eficácia do compressor, realizamos testes comparativos com diversos tipos de arquivos. Comparamos o desempenho do nosso compressor (\texttt{HuffmanCompressor}) com ferramentas de compressão padrão do mercado, como \texttt{zip} e \texttt{gzip}.

A taxa de compressão foi calculada usando a fórmula:
\[ \text{Taxa de Compressão} = 1 - \left( \frac{\text{Tamanho Comprimido}}{\text{Tamanho Original}} \right) \]

\subsection{Tabela de Resultados}

\begin{table}[h!]
\centering
\caption{Comparação da Taxa de Compressão}
\label{tab:compressao}
\begin{tabular}{|l|c|l|c|c|}
\hline
\textbf{Arquivo de Teste} & \textbf{Original} & \textbf{Ferramenta} & \textbf{Comprimido} & \textbf{Taxa (\%)} \\ \hline
\textbf{C++ (complexo)} & 7.1 KB & \textbf{HuffmanCompressor} & \textbf{4.9 KB} & \textbf{31.0\%} \\
\texttt{compressor.cpp} & 7.1 KB & zip & 5.3 KB & 25.4\% \\ \hline
\textbf{Texto Simples} & 10.2 KB & HuffmanCompressor & 6.1 KB & 40.2\% \\
\texttt{texto.txt} & 10.2 KB & \textbf{zip (DEFLATE)} & \textbf{5.5 KB} & \textbf{46.1\%} \\ \hline
\textbf{C++ (simples)} & 312 B & HuffmanCompressor & 283 B & 9.3\% \\
\texttt{simple\_code.cpp} & 312 B & \textbf{gzip} & \textbf{239 B} & \textbf{23.4\%} \\ \hline
\textbf{Java (médio)} & 980 B & HuffmanCompressor & 631 B & 35.6\% \\
\texttt{medium\_code.java} & 980 B & \textbf{gzip} & \textbf{347 B} & \textbf{64.6\%} \\ \hline
\textbf{JavaScript (médio)} & 3.9 KB & HuffmanCompressor & 2.7 KB & 30.6\% \\
\texttt{medium\_code.js} & 3.9 KB & \textbf{gzip} & \textbf{1.4 KB} & \textbf{63.2\%} \\ \hline
\textbf{TypeScript (simples)} & 431 B & HuffmanCompressor & 350 B & 18.8\% \\
\texttt{simple\_code.ts} & 431 B & \textbf{gzip} & \textbf{292 B} & \textbf{32.3\%} \\ \hline
\end{tabular}
\end{table}

\subsection{Análise dos Resultados}

Os resultados experimentais confirmam a hipótese central do projeto:
\begin{enumerate}
    \item \textbf{Desempenho em Código-Fonte C++}: O \texttt{HuffmanCompressor} foi superior ao \texttt{zip} na compressão do arquivo de código-fonte C++ mais complexo. A taxa de compressão de 31.0\% contra 25.4\% do \texttt{zip} demonstra a vantagem da nossa abordagem. Ao tratar palavras-chave como \texttt{std::string}, \texttt{vector}, \texttt{if}, e \texttt{while} como símbolos únicos, nosso algoritmo atribui a eles códigos binários muito curtos.
    
    \item \textbf{Desempenho em Texto Simples}: Para o arquivo de texto comum, o \texttt{zip} obteve um resultado superior. Isso ocorre porque algoritmos como o DEFLATE, usado pelo \texttt{zip}, combinam a codificação de Huffman com o algoritmo LZ77, que é extremamente eficaz em encontrar e substituir sequências de caracteres repetidas.
    
    \item \textbf{Desempenho em Outros Códigos-Fonte}: Os testes com códigos em outras linguagens, usando uma tabela de frequência treinada apenas com C++, revelaram um insight interessante. Para arquivos pequenos ou linguagens com sintaxe muito diferente, o custo de serializar a árvore de Huffman e a falta de uma tabela de frequência otimizada fizeram com que ferramentas padrão como \texttt{gzip} tivessem um desempenho superior. Isso sugere que, para obter a máxima eficiência, a tabela de frequência deve ser treinada com um corpus representativo da linguagem-alvo.
\end{enumerate}

\section{Conclusão}

O projeto foi concluído com sucesso, resultando em uma ferramenta de compressão robusta. A análise de desempenho mostrou que a especialização da tabela de frequência para incluir tokens de múltiplos caracteres é uma estratégia vitoriosa para a compressão de código-fonte. Embora não supere os compressores de uso geral em todos os cenários, o projeto prova que podemos sim, fazer algo semelhante que traga resultados bons. 

\end{document}
